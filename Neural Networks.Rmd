---
title: "Reti Neurali"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, comment = "")
```

### Funzione di Costo

$$ J(w) = \sum_{i=1}^{n}[y_i - f(x_i,w)]^2 + \lambda \sum_{j=1}^{p}w_j^2 $$

dove $\lambda \sum_{j=1}^{p}w_j^2$ equivale a dire che $\sum_{j=1}^{p}w_j^2 \leq s$. 
Se $\lambda \to \infty$ allora $s \to 0$ ($w \to 0$); se $\lambda \to 0$ allora $s \to \infty$ ($w \to OLS$).

### Funzioni di Attivazione

### Aggiornamento Weights

$$ w_j^{(t+1)} = w_j^{(t)} - \eta \frac{\partial{J(w)}}{\partial{w_j}} Optimizer - \lambda \eta w_j^{(t)} $$

- $\begin{aligned} 
    w_j^{(t)}: 
    \end{aligned}$ peso j-esimo al tempo t 

- $\begin{aligned} 
    \eta: 
    \end{aligned}$ learning rate 
    
- $\begin{aligned} 
    \frac{\partial{J(W)}}{\partial{w_j}}: 
    \end{aligned}$
    derivata parziale prima della funzione di costo (non penalizzata) rispetto a $w_j$
    
- $\begin{aligned} 
    Optimizer:
    \end{aligned}$ algoritmo di ottimizzazione del gradient-descent (momentum, adam, adagrad)

- $\begin{aligned} 
    \lambda:
    \end{aligned}$ weight decay 


### Back-Propagation

### Ottimizzatori

### Iperparametri


